{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dacon_universe(final)",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPtMYAC7j+r7zP8Ymh/vIF2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sinjy1203/deep-learning/blob/master/dacon_universe(final).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlaEC4hsuxoq",
        "colab_type": "code",
        "outputId": "2b63ec79-8858-44d8-c235-5b4659a4b23f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohgbsUMKu8vX",
        "colab_type": "code",
        "outputId": "6ab4f106-efdb-41f2-ca0c-fd9c7de66492",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "train = pd.read_csv('/content/gdrive/My Drive/dacon stage 2 data/train.csv', index_col=0)\n",
        "test = pd.read_csv('/content/gdrive/My Drive/dacon stage 2 data/test.csv', index_col=0)\n",
        "sample_submission = pd.read_csv('/content/gdrive/My Drive/dacon stage 2 data/sample_submission.csv', index_col=0)\n",
        "column_number = {}\n",
        "for i, column in enumerate(sample_submission.columns):\n",
        "    column_number[column] = i\n",
        "\n",
        "def to_number(x, dic):\n",
        "    return dic[x]\n",
        "\n",
        "train['type_num'] = train['type'].apply(lambda x: to_number(x, column_number))\n",
        "#######\n",
        "index = train.columns\n",
        "index_list = []\n",
        "for i in index:\n",
        "  index_list.append(i)\n",
        "index_list = np.array(index_list)  \n",
        "index_list_drop = index_list[(index_list != 'type')]\n",
        "index_list_drop2 = index_list_drop[(index_list_drop != 'type_num')]\n",
        "index_list_drop3 = index_list_drop2[(index_list_drop2 != 'fiberID')]\n",
        "index_list_drop3 = [x for x in index_list_drop3]\n",
        "std = train.std(0)\n",
        "for i, name in enumerate(index_list_drop3):\n",
        "  train = train[train[name] >= train[name].mean(0) -2 * std[name]]\n",
        "  train = train[train[name] <= train[name].mean(0) +2 * std[name]]\n",
        "train_x = train.drop(columns=['type', 'type_num'], axis=1)\n",
        "train_y = train['type_num']\n",
        "test_x = test\n",
        "cat_x = train_x[\"fiberID\"]\n",
        "train_x = train_x.drop(columns='fiberID', axis=1)\n",
        "std_scaler = StandardScaler()\n",
        "train_x = std_scaler.fit_transform(train_x)\n",
        "train_x = pd.DataFrame(data=train_x, columns=index_list_drop3)\n",
        "# plt.figure(figsize=(15, 10))\n",
        "# train_x.boxplot(column=index_list_drop3)\n",
        "######\n",
        "\n",
        "cat_x_encoded, cat_x_categories = cat_x.factorize()\n",
        "encoder = OneHotEncoder(categories='auto')\n",
        "cat_x_1hot = encoder.fit_transform(cat_x_encoded.reshape(-1,1))\n",
        "cat_x_1hot_array = cat_x_1hot.toarray()\n",
        "\n",
        "combine_train_x = np.c_[train_x, cat_x_1hot_array]\n",
        "x_train = combine_train_x\n",
        "print(x_train[:3])\n",
        "\n",
        "cat_test_x = test_x[\"fiberID\"]\n",
        "cat_test_x_encoded, cat_test_x_categories = cat_test_x.factorize()\n",
        "cat_test_x_1hot = encoder.transform(cat_test_x_encoded.reshape(-1,1))\n",
        "cat_test_x_1hot_array = cat_test_x_1hot.toarray()\n",
        "drop_test_x1 = test_x.drop(\"fiberID\", axis=1)\n",
        "drop_test_x = std_scaler.transform(drop_test_x1)\n",
        "combine_test_x = np.c_[drop_test_x, cat_test_x_1hot_array]\n",
        "x_test = combine_test_x\n",
        "print(len(x_train))\n",
        "\n",
        "tf.set_random_seed(777)\n",
        "y_train = train_y\n",
        "y_train = np.array(y_train)\n",
        "y_train = y_train.reshape(-1, 1)\n",
        "\n",
        "x_train1 = x_train[:199000]\n",
        "x_train2 = x_train[199000:]\n",
        "y_train1 = y_train[:199000]\n",
        "y_train2 = y_train[199000:]\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None, 1020])\n",
        "y = tf.placeholder(tf.int32, [None, 1])\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "y_one_hot = tf.one_hot(y, 19)\n",
        "y_one_hot = tf.reshape(y_one_hot, [-1, 19])\n",
        "\n",
        "W1 = tf.get_variable(\"w11111111\", shape=[1020, 1054], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b1 = tf.Variable(tf.random_normal([1054]))\n",
        "L1 = tf.nn.leaky_relu(tf.matmul(x, W1) + b1)\n",
        "L1 = tf.nn.dropout(L1, keep_prob= keep_prob)\n",
        "\n",
        "W2 = tf.get_variable(\"w22222222\", shape=[1054, 1054], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b2 = tf.Variable(tf.random_normal([1054]))\n",
        "L2 = tf.nn.leaky_relu(tf.matmul(L1, W2) + b2)\n",
        "L2 = tf.nn.dropout(L2, keep_prob= keep_prob)\n",
        "\n",
        "W3 = tf.get_variable(\"w33333333\", shape=[1054, 1054], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b3 = tf.Variable(tf.random_normal([1054]))\n",
        "L3 = tf.nn.leaky_relu(tf.matmul(L2, W3) + b3)\n",
        "L3 = tf.nn.dropout(L3, keep_prob= keep_prob)\n",
        "\n",
        "W4 = tf.get_variable(\"w44444444\", shape=[1054, 1054], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b4 = tf.Variable(tf.random_normal([1054]))\n",
        "L4 = tf.nn.leaky_relu(tf.matmul(L3, W4) + b4)\n",
        "L4 = tf.nn.dropout(L4, keep_prob= keep_prob)\n",
        "\n",
        "W5 = tf.get_variable(\"w55555555\", shape=[1054, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b5 = tf.Variable(tf.random_normal([512]))\n",
        "L5 = tf.nn.leaky_relu(tf.matmul(L4, W5) + b5)\n",
        "L5 = tf.nn.dropout(L5, keep_prob= keep_prob)\n",
        "\n",
        "W6 = tf.get_variable(\"w66666666\", shape=[512, 512], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b6 = tf.Variable(tf.random_normal([512]))\n",
        "L6 = tf.nn.leaky_relu(tf.matmul(L5, W6) + b6)\n",
        "L6 = tf.nn.dropout(L6, keep_prob= keep_prob)\n",
        "\n",
        "W7 = tf.get_variable(\"w77777777\",shape=[512, 19], initializer=tf.contrib.layers.xavier_initializer())\n",
        "b7 = tf.Variable(tf.random_normal([19]))\n",
        "logits = tf.matmul(L6, W7) + b7\n",
        "hypothesis = tf.nn.softmax(logits)\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_one_hot))\n",
        "train = tf.train.AdamOptimizer(0.005).minimize(cost)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(hypothesis,1), tf.argmax(y_one_hot,1)), tf.float32))\n",
        "\n",
        "total_epochs = 10\n",
        "batch_size = 1000\n",
        "number = int(199000 / batch_size)\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for epochs in range(total_epochs):\n",
        "    avg_cost = 0\n",
        "    for i in range(number):\n",
        "      xs = x_train1[i * batch_size:(i + 1) * batch_size]\n",
        "      ys = y_train1[i * batch_size:(i + 1) * batch_size]\n",
        "      _, cost_val = sess.run([train, cost], feed_dict={x: xs, y: ys, keep_prob: 0.5})\n",
        "      if(i == (number - 1)):\n",
        "        _, cost_val = sess.run([train, cost], feed_dict={x: x_train2, y: y_train2, keep_prob: 0.5})\n",
        "      avg_cost += cost_val / (number + 1)\n",
        "    if(epochs % 1 == 0):\n",
        "      print(epochs, '  ', avg_cost)\n",
        "\n",
        "  y_pred = sess.run(hypothesis, feed_dict={x: x_test, keep_prob: 1})\n",
        "  print(y_pred)\n",
        "\n",
        "  submission = pd.DataFrame(data=y_pred, columns=sample_submission.columns, index=sample_submission.index)\n",
        "  submission.to_csv('/content/gdrive/My Drive/submission7.csv', index=True)\n",
        "  #####score = 0.88"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[[ 0.78596369  0.82628111  1.17212226 ...  0.          0.\n",
            "   0.        ]\n",
            " [ 0.15910134  0.44512825  0.80453329 ...  0.          0.\n",
            "   0.        ]\n",
            " [-1.1110021  -1.65070398 -1.50942413 ...  0.          0.\n",
            "   0.        ]]\n",
            "199942\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-2-4880407727d4>:83: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-2-4880407727d4>:114: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "0    6.3215809977054604\n",
            "1    1.0907246562838555\n",
            "2    0.9021916526556015\n",
            "3    0.8063376608490944\n",
            "4    0.7444964668154713\n",
            "5    0.7072206550836566\n",
            "6    0.6783923557400702\n",
            "7    0.6655962371826175\n",
            "8    0.6587769481539721\n",
            "9    0.663207733929157\n",
            "[[1.93684158e-07 3.80202636e-07 2.15508719e-03 ... 7.83261421e-05\n",
            "  2.40512254e-05 1.85355548e-05]\n",
            " [8.10260985e-07 1.42624970e-07 1.06992815e-08 ... 4.64403804e-08\n",
            "  9.61307922e-09 3.98772012e-04]\n",
            " [7.31436186e-04 8.23829847e-04 9.64474111e-06 ... 7.70549872e-04\n",
            "  3.03497254e-05 4.83848737e-04]\n",
            " ...\n",
            " [4.86127988e-07 5.58705224e-08 1.57534963e-09 ... 4.46093873e-09\n",
            "  4.22110036e-09 1.15014554e-03]\n",
            " [1.19011964e-13 7.85612629e-16 6.45588859e-17 ... 1.26965995e-17\n",
            "  1.35178888e-16 3.15895051e-07]\n",
            " [3.60534876e-04 1.16524068e-04 7.46531350e-06 ... 5.10961854e-06\n",
            "  7.37421124e-06 2.79253843e-04]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}