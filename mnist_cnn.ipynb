{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist_cnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMTaFbAHvappowFWWN1A9sh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sinjy1203/deep-learning/blob/master/mnist_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ta4zfzhRQ1o8",
        "colab_type": "code",
        "outputId": "885204a1-9fd5-4582-8043-3cc2e28837e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNUfnnrQSfxZ",
        "colab_type": "code",
        "outputId": "fb859c07-aac9-46b2-e991-c14f8ca5e8d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoQv1ypNSmTb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train1 = x_train.reshape(-1, 28, 28, 1).astype(np.float32)\n",
        "y_train1 = y_train.astype(np.int32)\n",
        "x_test1 = x_test.reshape(-1, 28, 28, 1).astype(np.float32)\n",
        "y_test1 = y_test.astype(np.int32)\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train2, x_val = train_test_split(x_train1, test_size=0.2, random_state=42)\n",
        "y_train2, y_val = train_test_split(y_train1, test_size=0.2, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AXJVw3BVoYq",
        "colab_type": "code",
        "outputId": "ae108808-fa89-4b1a-d573-f02f4162a7d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "source": [
        "import time\n",
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "def shuffle_batch(x, y, batch_size):\n",
        "  rnd_idx = np.random.permutation(len(x))\n",
        "  n_batch = len(x) // batch_size\n",
        "  for batch_idx in np.array_split(rnd_idx, n_batch):\n",
        "    xs, ys = x[batch_idx], y[batch_idx]\n",
        "    yield xs, ys\n",
        "\n",
        "start = time.time()\n",
        "x = tf.placeholder(dtype=tf.float32, shape=(None, 28, 28, 1), name='x')\n",
        "y = tf.placeholder(dtype=tf.int32, shape=(None), name='y')\n",
        "training = tf.placeholder_with_default(False, shape=[])\n",
        "he_init = tf.variance_scaling_initializer()\n",
        "conv1 = tf.layers.conv2d(x, filters=10, kernel_size=3, strides=[1,1], activation=tf.nn.elu, padding='SAME')\n",
        "max_pool1 = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
        "conv2 = tf.layers.conv2d(max_pool1, filters=50, kernel_size=3, strides=(1,1), activation=tf.nn.elu, padding='SAME')\n",
        "max_pool2 = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
        "\n",
        "x_reshape = tf.reshape(max_pool2, shape=(-1, 7*7*50))\n",
        "layer = x_reshape\n",
        "n_layers = 3\n",
        "\n",
        "\n",
        "for i in range(n_layers):\n",
        "  if i == n_layers - 1:\n",
        "    layer = tf.layers.dropout(layer, rate=0.5, training=training)\n",
        "    layer = tf.layers.dense(layer, 10, kernel_initializer=he_init)\n",
        "  else:\n",
        "    layer = tf.layers.dropout(layer, rate=0.5, training=training)\n",
        "    layer = tf.layers.dense(layer, 100, kernel_initializer=he_init)\n",
        "    layer = tf.layers.batch_normalization(layer, momentum=0.9, training=training)\n",
        "    layer = tf.nn.elu(layer)\n",
        "  \n",
        "  # layer = tf.layers.batch_normalization(layer, momentum=0.9, training=training, )\n",
        "\n",
        "logits = layer\n",
        "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits, name='xentropy')\n",
        "loss = tf.reduce_mean(xentropy)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
        "training_op = optimizer.minimize(loss)\n",
        "\n",
        "correct = tf.nn.in_top_k(logits, y, 1)\n",
        "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# file_writer = tf.summary.FileWriter(\"./log\", tf.get_default_graph())\n",
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  for epoch in range(51):\n",
        "    for xs, ys in shuffle_batch(x_train2, y_train2, 100):\n",
        "      _, cost_val, __ = sess.run([training_op, loss, extra_update_ops], feed_dict={x: xs, y: ys, training: True})\n",
        "    if epoch % 10 == 0:\n",
        "      accuracy_val = accuracy.eval(feed_dict={x: x_val, y: y_val})\n",
        "      print(accuracy_val, cost_val)\n",
        "  accuracy_test = accuracy.eval(feed_dict={x: x_test1, y: y_test1})\n",
        "  print(\"정확도: \", accuracy_test) \n",
        "print(\"걸린 시간: \", time.time() - start)####정확도 99.3%"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-860cd4d2a5ab>:18: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-4-860cd4d2a5ab>:33: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "WARNING:tensorflow:From <ipython-input-4-860cd4d2a5ab>:34: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From <ipython-input-4-860cd4d2a5ab>:35: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
            "0.98 0.15386526\n",
            "0.99116665 0.06706425\n",
            "0.99175 0.023364717\n",
            "0.99258333 0.04016701\n",
            "0.99325 0.018633695\n",
            "0.99275 0.0035382395\n",
            "정확도:  0.9933\n",
            "걸린 시간:  135.43223524093628\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}