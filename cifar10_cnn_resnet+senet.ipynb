{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar10_cnn_resnet+senet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPuoXX/iCu3yX53tyS4y39r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sinjy1203/deep-learning/blob/master/cifar10_cnn_resnet%2Bsenet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkImopAbMX-A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "4fef463d-dbd9-4062-8b37-88bcfff5fb23"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahV-CONeMdaH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "614d306d-87db-44e1-ebe2-62c303af665d"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras.datasets import cifar10\n",
        "from functools import partial\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train.shape, x_test.shape\n",
        "x_train = x_train.astype(np.float32) / 255.0\n",
        "x_test = x_test.astype(np.float32) / 255.0\n",
        "y_train = y_train.astype(np.int32).reshape(-1)\n",
        "y_test = y_test.astype(np.int32).reshape(-1)\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_val = train_test_split(x_train, test_size=0.2, random_state=42)\n",
        "y_train, y_val = train_test_split(y_train, test_size=0.2, random_state=42)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr7oZiF8LX8w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "outputId": "0bb57d6a-1fde-4907-f302-4b04cc601702"
      },
      "source": [
        "import time\n",
        "start = time.time()\n",
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "def shuffle_batch(x, y, batch_size):\n",
        "  rnd_idx = np.random.permutation(len(x))\n",
        "  n_batch = len(x) // batch_size\n",
        "  for batch_idx in np.array_split(rnd_idx, n_batch):\n",
        "    xs, ys = x[batch_idx], y[batch_idx]\n",
        "    yield xs, ys\n",
        "\n",
        "name = ['drop', 'layer', 'bn', 'act']\n",
        "name_list = []\n",
        "for i in range(4):\n",
        "  bb = [b + str(i) for b in name]\n",
        "  name_list.append(bb)\n",
        "  bb = []\n",
        "\n",
        "n_layers=4\n",
        "n_epochs = 101\n",
        "batch_size = 1000\n",
        "x = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3], name=\"x\")\n",
        "y = tf.placeholder(dtype=tf.int32, shape=[None], name=\"y\")\n",
        "training = tf.placeholder_with_default(False, shape=[], name=\"training\")\n",
        "he_init = tf.variance_scaling_initializer()\n",
        "\n",
        "##########partial########\n",
        "conv = partial(tf.layers.conv2d, kernel_size=3, padding='SAME', kernel_initializer=he_init)\n",
        "conv_re = partial(tf.layers.conv2d, kernel_size=1, strides=2, padding='SAME', kernel_initializer=he_init)\n",
        "max_pool = partial(tf.layers.max_pooling2d, padding='SAME', strides=2)\n",
        "avg_pool = partial(tf.layers.average_pooling2d, padding='VALID', strides=1)\n",
        "bn = partial(tf.layers.batch_normalization, training=training, momentum=0.9)\n",
        "drop = partial(tf.layers.dropout, rate=0.5, training=training)\n",
        "\n",
        "##########convolution########\n",
        "conv1 = conv(x, filters=64, strides=1, name='conv1')\n",
        "bn1 = bn(conv1, name='bn1')\n",
        "act1 = tf.nn.relu(bn1)\n",
        "max_pool1 = max_pool(act1, pool_size=3, name='max_pool1')\n",
        "\n",
        "conv2 = conv(max_pool1, filters=64, strides=1, name='conv2')\n",
        "bn2 = bn(conv2, name='bn2')\n",
        "act2 = tf.nn.relu(bn2, name='act2')\n",
        "conv2_2 = conv(act2, filters=64, strides=1, name='conv2_2')\n",
        "bn2_2 = bn(conv2_2, name='bn2_2')\n",
        "##se block\n",
        "avg_pool2 = avg_pool(bn2_2, pool_size=16, name='avg_pool2')\n",
        "se_block2_1 = tf.layers.dense(avg_pool2, 64/2, kernel_initializer=he_init, activation=tf.nn.relu, name='se_block2_1')\n",
        "se_block2_2 = tf.layers.dense(se_block2_1, 64, kernel_initializer=he_init, activation=tf.nn.sigmoid, name='se_block2_2')\n",
        "res2 = tf.add(max_pool1, bn2_2*se_block2_2, name='res2')\n",
        "act2_2 = tf.nn.relu(res2, name='act2_2')\n",
        "##\n",
        "\n",
        "conv3 = conv(act2_2, filters=64, strides=1, name='conv3')\n",
        "bn3 = bn(conv3, name='bn3')\n",
        "act3 = tf.nn.relu(bn3, name='act3')\n",
        "conv3_2 = conv(act3, filters=64, strides=1, name='conv3_2')\n",
        "bn3_2 = bn(conv3_2, name='bn3_2')\n",
        "##se block\n",
        "avg_pool3 = avg_pool(bn3_2, pool_size=16, name='avg_pool3')\n",
        "se_block3_1 = tf.layers.dense(avg_pool3, 64/2, kernel_initializer=he_init, activation=tf.nn.relu, name='se_block3_1')\n",
        "se_block3_2 = tf.layers.dense(se_block3_1, 64, kernel_initializer=he_init, activation=tf.nn.sigmoid, name='se_block3_2')\n",
        "res3 = tf.add(act2_2, bn3_2*se_block3_2, name='res3')\n",
        "act3_2 = tf.nn.relu(res3, name='act3_2')\n",
        "#######\n",
        "\n",
        "conv4 = conv(act3_2, filters=128, strides=2, name='conv4')\n",
        "bn4 = bn(conv4, name='bn4')\n",
        "act4 = tf.nn.relu(bn4, name='act4')\n",
        "conv4_2 = conv(act4, filters=128, strides=1, name='conv4_2')\n",
        "bn4_2 = bn(conv4_2, name='bn4_2')\n",
        "avg_pool4 = avg_pool(bn4_2, pool_size=8, name='avg_pool4')\n",
        "se_block4_1 = tf.layers.dense(avg_pool4, 128/2, kernel_initializer=he_init, activation=tf.nn.relu, name='se_block4_1')\n",
        "se_block4_2 = tf.layers.dense(se_block4_1, 128, kernel_initializer=he_init, activation=tf.nn.sigmoid, name='se_block4_2')\n",
        "conv4_re = conv_re(act3_2, filters=128, name='conv4_re')\n",
        "bn4_re = bn(conv4_re, name='bn4_re')\n",
        "res4 = tf.add(bn4_re, bn4_2*se_block4_2, name='res4')\n",
        "act4_2 = tf.nn.relu(res4, name='act4_2')\n",
        "##\n",
        "conv5 = conv(act4_2, filters=128, strides=1, name='conv5')\n",
        "bn5 = bn(conv5, name='bn5')\n",
        "act5 = tf.nn.relu(bn5, name='act5')\n",
        "conv5_2 = conv(act5, filters=128, strides=1, name='conv5_2')\n",
        "bn5_2 = bn(conv5_2, name='bn5_2')\n",
        "avg_pool5 = avg_pool(bn5_2, pool_size=8, name='avg_pool5')\n",
        "se_block5_1 = tf.layers.dense(avg_pool5, 128/2, kernel_initializer=he_init, activation=tf.nn.relu, name='se_block5_1')\n",
        "se_block5_2 = tf.layers.dense(se_block5_1, 128, kernel_initializer=he_init, activation=tf.nn.sigmoid, name='se_block5_2')\n",
        "res5 = tf.add(act4_2, bn5_2*se_block5_2, name='res5')\n",
        "act5_2 = tf.nn.relu(res5, name='act5_2')\n",
        "#######\n",
        "\n",
        "conv6 = conv(act5_2, filters=256, strides=2, name='conv6')\n",
        "bn6 = bn(conv6, name='bn6')\n",
        "act6 = tf.nn.relu(bn6, name='act6')\n",
        "conv6_2 = conv(act6, filters=256, strides=1, name='conv6_2')\n",
        "bn6_2 = bn(conv6_2, name='bn6_2')\n",
        "avg_pool6 = avg_pool(bn6_2, pool_size=4, name='avg_pool5')\n",
        "se_block6_1 = tf.layers.dense(avg_pool6, 256/2, kernel_initializer=he_init, activation=tf.nn.relu, name='se_block6_1')\n",
        "se_block6_2 = tf.layers.dense(se_block6_1, 256, kernel_initializer=he_init, activation=tf.nn.sigmoid, name='se_block6_2')\n",
        "conv6_re = conv_re(act5_2, filters=256, name='conv6_re')\n",
        "bn6_re = bn(conv6_re, name='bn6_re')\n",
        "res6 = tf.add(bn6_re, bn6_2*se_block6_2, name='res6')\n",
        "act6_2 = tf.nn.relu(res6, name='act6_2')\n",
        "##\n",
        "conv7 = conv(act6_2, filters=256, strides=1, name='conv7')\n",
        "bn7 = bn(conv7, name='bn7')\n",
        "act7 = tf.nn.relu(bn7, name='act7')\n",
        "conv7_2 = conv(act7, filters=256, strides=1, name='conv7_2')\n",
        "bn7_2 = bn(conv7_2, name='bn7_2')\n",
        "avg_pool7 = avg_pool(bn7_2, pool_size=4, name='avg_pool7')\n",
        "se_block7_1 = tf.layers.dense(avg_pool7, 256/2, kernel_initializer=he_init, activation=tf.nn.relu, name='se_block7_1')\n",
        "se_block7_2 = tf.layers.dense(se_block7_1, 256, kernel_initializer=he_init, activation=tf.nn.sigmoid, name='se_block7_2')\n",
        "res7 = tf.add(act6_2, bn7_2*se_block7_2, name='res7')\n",
        "act7_2 = tf.nn.relu(res7, name='act7_2')\n",
        "\n",
        "###############\n",
        "\n",
        "avg_pool = tf.layers.average_pooling2d(act7_2, pool_size=4, strides=(1,1), padding='VALID', name='avg_pool')\n",
        "reshape = tf.reshape(avg_pool, shape=[-1, 256])\n",
        "\n",
        "####fully connected#####\n",
        "drop5 = tf.layers.dropout(reshape, rate=0.5, training=training, name=\"drop5\")\n",
        "layer5 = tf.layers.dense(drop5, 10, kernel_initializer=he_init, name=\"layer5\")\n",
        "################################\n",
        "\n",
        "logits = layer5\n",
        "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits, name=\"xentropy\")\n",
        "loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
        "optimizer = tf.train.AdamOptimizer(0.001, name=\"optimizer\")\n",
        "training_op = optimizer.minimize(loss, name=\"training_op\")\n",
        "\n",
        "correct = tf.nn.in_top_k(logits, y, 1)\n",
        "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()\n",
        "cost_sum = []\n",
        "file_writer = tf.summary.FileWriter(\"./\", tf.get_default_graph())\n",
        "with tf.Session() as sess:\n",
        "  init.run()\n",
        "  for epoch in range(n_epochs):\n",
        "    for xs, ys in shuffle_batch(x_train, y_train, batch_size):\n",
        "      _, cost, __ = sess.run([training_op, loss, extra_update_ops], feed_dict={x: xs, y: ys, training: True})\n",
        "      cost_sum.append(cost)\n",
        "    cost_avg = sum(cost_sum) / len(cost_sum)\n",
        "    if epoch % 10 == 0:\n",
        "      acc_val = sess.run(accuracy, feed_dict={x: x_val, y: y_val})\n",
        "      print(\"{}번째\\t손실: {}\\t테스트정확도: {}\".format(epoch, cost_avg, acc_val))\n",
        "    cost_avg = 0.0\n",
        "  save_path = saver.save(sess, \"./model\")\n",
        "  print(\"걸린시간: \", time.time()-start)\n",
        "# with tf.Session() as sess:\n",
        "#   init.run()\n",
        "#   a = conv3.eval(feed_dict={x: x_test, y: y_test})\n",
        "#   print(a.shape)\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-8f183f789814>:38: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-3-8f183f789814>:39: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
            "WARNING:tensorflow:From <ipython-input-3-8f183f789814>:41: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.MaxPooling2D instead.\n",
            "WARNING:tensorflow:From <ipython-input-3-8f183f789814>:49: average_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.AveragePooling2D instead.\n",
            "WARNING:tensorflow:From <ipython-input-3-8f183f789814>:50: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From <ipython-input-3-8f183f789814>:124: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "0번째\t손실: 1.5128108203411101\t테스트정확도: 0.43299999833106995\n",
            "10번째\t손실: 0.5134766824035482\t테스트정확도: 0.7060999870300293\n",
            "20번째\t손실: 0.29100308837147343\t테스트정확도: 0.7533000111579895\n",
            "30번째\t손실: 0.206205931843649\t테스트정확도: 0.7506999969482422\n",
            "40번째\t손실: 0.16108229042076302\t테스트정확도: 0.7646999955177307\n",
            "50번째\t손실: 0.1332953241120741\t테스트정확도: 0.7634999752044678\n",
            "60번째\t손실: 0.11398837121716915\t테스트정확도: 0.7631000280380249\n",
            "70번째\t손실: 0.09989070265183331\t테스트정확도: 0.7603999972343445\n",
            "80번째\t손실: 0.08900129709146301\t테스트정확도: 0.767799973487854\n",
            "90번째\t손실: 0.08048897131276125\t테스트정확도: 0.7135000228881836\n",
            "100번째\t손실: 0.07309760665957042\t테스트정확도: 0.7998999953269958\n",
            "걸린시간:  742.4038095474243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o87eNWPZMaMy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "a100d699-188b-4b15-cd41-13a5fbc4703c"
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, \"./model\")\n",
        "  acc_test = accuracy.eval(feed_dict={x: x_test, y: y_test})\n",
        "  print(acc_test)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./model\n",
            "0.7939\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}