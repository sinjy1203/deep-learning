{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "handson_mnist.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPU9/8BaG8E/Yc3f5kHBUkf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sinjy1203/deep-learning/blob/master/handson_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uQFqRX3V-rk",
        "colab_type": "code",
        "outputId": "1b04d7e0-81c7-4a80-dc19-6262f4e0e846",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhLjyRfif5-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28KlOP3ag2Td",
        "colab_type": "code",
        "outputId": "7feb165b-55bd-45b6-afbd-95a783f026ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "x_train, x_test = x_train.astype(np.float32).reshape(-1, 28*28), x_test.astype(np.float32).reshape(-1, 28*28)\n",
        "y_train, y_test = y_train.astype(np.int32), y_test.astype(np.int32)\n",
        "y_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyBX7z5PkyN8",
        "colab_type": "code",
        "outputId": "e1caa6a2-ca86-47d0-eac3-25a1172704ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "from functools import partial\n",
        "import time\n",
        "start = time.time()\n",
        "def shuffle_batch(X, y, batch_size):\n",
        "    rnd_idx = np.random.permutation(len(X))\n",
        "    n_batches = len(X) // batch_size\n",
        "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
        "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
        "        yield X_batch, y_batch\n",
        "\n",
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(42)\n",
        "np.random.seed(42)\n",
        "x = tf.placeholder(dtype=tf.float32, shape=(None, 784), name='x')\n",
        "y = tf.placeholder(dtype=tf.int32, shape=(None), name='y')\n",
        "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
        "dropout_rate = 0.5\n",
        "\n",
        "he_init = tf.variance_scaling_initializer()\n",
        "\n",
        "# layer1 = tf.layers.dense(x, 200, activation=tf.nn.relu, kernel_initializer=he_init, name='layer1')\n",
        "# layer2 = tf.layers.dense(layer1, 200, activation=tf.nn.elu, kernel_initializer=he_init, name='layer2')\n",
        "# layer3 = tf.layers.dense(layer2, 200, activation=tf.nn.elu, kernel_initializer=he_init, name='layer3')\n",
        "# layer4 = tf.layers.dense(layer3, 200, activation=tf.nn.elu, kernel_initializer=he_init, name='layer4')\n",
        "# layer5 = tf.layers.dense(layer4, 200, activation=tf.nn.elu, kernel_initializer=he_init, name='layer5')\n",
        "# logits = tf.layers.dense(layer5, 10, kernel_initializer=he_init)\n",
        "# # loss: 0.00038, time: 216 acc: 0.966, learning_rate: 0.001\n",
        "\n",
        "my_batch_norm_layer = partial(tf.layers.batch_normalization, training=training, momentum=0.9)\n",
        "\n",
        "x_drop = tf.layers.dropout(x, dropout_rate, training=training)\n",
        "\n",
        "layer1 = tf.layers.dense(x_drop, 200, kernel_initializer=he_init, name='layer1')\n",
        "bn1 = my_batch_norm_layer(layer1)\n",
        "bn1_act = tf.nn.elu(bn1)\n",
        "layer1_drop = tf.layers.dropout(bn1_act, dropout_rate, training=training)\n",
        "\n",
        "layer2 = tf.layers.dense(layer1_drop, 200, kernel_initializer=he_init, name='layer2')\n",
        "bn2 = my_batch_norm_layer(layer2)\n",
        "bn2_act = tf.nn.elu(bn2)\n",
        "layer2_drop = tf.layers.dropout(bn2_act, dropout_rate, training=training)\n",
        "\n",
        "layer3 = tf.layers.dense(layer2_drop, 200, kernel_initializer=he_init, name='layer3')\n",
        "bn3 = my_batch_norm_layer(layer3)\n",
        "bn3_act = tf.nn.elu(bn3)\n",
        "layer3_drop = tf.layers.dropout(bn3_act, dropout_rate, training=training)\n",
        "\n",
        "layer4 = tf.layers.dense(layer3_drop, 200, kernel_initializer=he_init, name='layer4')\n",
        "bn4 = my_batch_norm_layer(layer4)\n",
        "bn4_act = tf.nn.elu(bn4)\n",
        "layer4_drop = tf.layers.dropout(bn4_act, dropout_rate, training=training)\n",
        "\n",
        "layer5 = tf.layers.dense(layer4_drop, 200, kernel_initializer=he_init, name='layer5')\n",
        "bn5 = my_batch_norm_layer(layer5)\n",
        "bn5_act = tf.nn.elu(bn5)\n",
        "layer5_drop = tf.layers.dropout(bn5_act, dropout_rate, training=training)\n",
        "\n",
        "logits_before = tf.layers.dense(layer5_drop, 10, kernel_initializer=he_init)\n",
        "logits = my_batch_norm_layer(logits_before)\n",
        "# loss: 0.0053, time: 110, acc: 0.9825, learning_rate: 0.05, epoch: 10\n",
        "\n",
        "pred = tf.nn.softmax(logits)\n",
        "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits=logits)\n",
        "loss = tf.reduce_mean(xentropy)\n",
        "# initial_learning_rate = 0.1\n",
        "# decay_steps = 10000\n",
        "# decay_rate = 0.1\n",
        "# global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
        "# learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, decay_steps, decay_rate)\n",
        "# optimizer = tf.train.GradientDescentOptimizer(0.05)\n",
        "# optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9, use_nesterov=True)\n",
        "# optimizer = tf.train.RMSPropOptimizer(learning_rate=0.05, momentum=0.9, decay=0.9, epsilon=1e-10)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
        "train_op = optimizer.minimize(loss)\n",
        "\n",
        "correct = tf.nn.in_top_k(logits, y, 1)\n",
        "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "n_epochs = 11\n",
        "batch_size = 50\n",
        "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  for n_epoch in range(n_epochs):\n",
        "    cost = 0.0\n",
        "    for i in range(np.int(len(x_train) / batch_size)):\n",
        "      xs = x_train[i * batch_size:i * batch_size + batch_size]\n",
        "      ys = y_train[i * batch_size:i * batch_size + batch_size]\n",
        "      _, cost_val, __ = sess.run([train_op, loss, extra_update_ops], feed_dict={x: xs, y: ys, training: True})\n",
        "      cost += cost_val / (len(x_train) / batch_size)\n",
        "    if n_epoch % 1 == 0:\n",
        "      print(n_epoch, cost)\n",
        "  save_path = saver.save(sess, \"/content/gdrive/My Drive/tmp/my_model3.ckpt\")\n",
        "print(\"time: \", time.time() - start)\n",
        "# a = 0\n",
        "# with tf.Session() as sess:\n",
        "#   sess.run(init)\n",
        "#   cost = 0\n",
        "#   for epoch in range(n_epochs):\n",
        "#         for x_batch, y_batch in shuffle_batch(x_train, y_train, batch_size):\n",
        "#             _, cost_val = sess.run([train_op, loss], feed_dict={x: x_batch, y: y_batch})\n",
        "#             #a += 1\n",
        "#             cost += cost_val / 1200\n",
        "#         if epoch % 10 == 0:\n",
        "#           print(epoch,\"  \", cost)\n",
        "#   save_path = saver.save(sess, \"/content/gdrive/My Drive/tmp/my_model3.ckpt\")\n",
        "  #print(a)\n",
        "# with tf.Session() as sess:\n",
        "#   sess.run(init)\n",
        "#   for n_epoch in range(n_epochs):\n",
        "#     _, cost = sess.run([train_op, loss], feed_dict={x: x_train, y: y_train})\n",
        "#     if n_epoch % 10 == 0:\n",
        "#       print(n_epoch, cost)\n",
        "#   save_path = saver.save(sess, \"/content/gdrive/My Drive/tmp/my_model3.ckpt\") 0.018"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 1.0210959591219826\n",
            "1 0.6528678620606659\n",
            "2 0.5696823270743084\n",
            "3 0.5254673164710401\n",
            "4 0.497938951291144\n",
            "5 0.46817267211154084\n",
            "6 0.45477097672720745\n",
            "7 0.43868895040825034\n",
            "8 0.4235622403336066\n",
            "9 0.41553315779815075\n",
            "10 0.4006539055580896\n",
            "time:  124.6114866733551\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ET461dO8xEcT",
        "colab_type": "code",
        "outputId": "47608b64-77a8-4362-e6e8-748b41ede183",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, \"/content/gdrive/My Drive/tmp/my_model3.ckpt\")\n",
        "  acc = sess.run(accuracy, feed_dict={x: x_test, y: y_test})\n",
        "  print(acc, \"he 초기화+elu+배치정규화\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/tmp/my_model3.ckpt\n",
            "0.9519 he 초기화+elu+배치정규화\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}